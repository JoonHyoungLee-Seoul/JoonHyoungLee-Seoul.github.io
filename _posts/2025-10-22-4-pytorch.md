---
layout: single
title: "[PyTorch] 2. PyTorch Functions: Mathematical Operations and Tensor Manipulation" 
date: 2025-10-22
categories: Pytorch
#header:
  #overlay_image: /assets/images/docker-container-tech.svg
  #overlay_filter: 0.5
  #teaser: /assets/images/docker-container-tech.svg
  #caption: "Create Docker Image"
Typora-root-url: ../
---

# PyTorch Functions: Mathematical Operations and Tensor Manipulation

This tutorial covers essential PyTorch functions for tensor manipulation, mathematical operations, and data transformations that are fundamental for deep learning workflows.

## Random tensor generation

PyTorch provides functions to generate random tensors from different distributions.

```python
import torch

A = torch.randn(3,3) # Normal distribution (n for normal)
B = torch.rand(3,3)  # Uniform distribution

print(A)
print(B)
```

**Output:**
```
tensor([[ 3.1719, -0.4043, -1.2808],
        [-0.1339,  0.6801,  1.0268],
        [-0.0902,  0.0180,  0.2038]])
tensor([[0.3645, 0.4637, 0.0314],
        [0.7558, 0.9402, 0.6517],
        [0.0799, 0.9369, 0.6067]])
```

## Mathematical functions

### Basic mathematical operations

```python
A = torch.randn(3,3)
print(A)
print(torch.abs(A))
print(torch.sqrt(torch.abs(A)))
print(torch.exp(A))
print(torch.log(torch.abs(A)))
print(torch.log(torch.exp(torch.tensor(1)))) # torch.exp(torch.tensor) = e^1
print(torch.log10(torch.tensor(10)))
print(torch.log2(torch.tensor(2)))
print(torch.round(A))
print(torch.round(A, decimals=2)) # Round to 2 decimal places
print(torch.floor(A)) # Round down
print(torch.ceil(A))  # Round up
```

### Trigonometric functions

```python
print(torch.sin(torch.tensor(torch.pi/6))) # type(torch.pi) <- float, but can be tensor with torch.tensor
print(torch.cos(torch.tensor(torch.pi/3)))
print(torch.tan(torch.tensor(torch.pi/4)))
print(torch.tanh(torch.tensor(-10)))

type(torch.tensor(1)/6)
```

**Output:**
```
tensor(0.5000)
tensor(0.5000)
tensor(1.0000)
tensor(-1.)

torch.Tensor
```

### Special values: NaN and Inf

```python
torch.nan # Not a number
print(torch.log(torch.tensor(-1)))
print(torch.isnan(torch.tensor([1,2,torch.nan,3,4])))
print(torch.isinf(torch.tensor([1,2,3,4,torch.inf])))
```

**Output:**
```
tensor(nan)
tensor([False, False,  True, False, False])
tensor([False, False, False, False,  True])
```

## Aggregate functions

### Max, min, and argmax

```python
A = torch.randn(3,4)
print(A)
print(torch.max(A))
print(torch.max(A, dim=0)) # check columns -> 4 outputs
print(torch.max(A, dim=1)) # check rows -> 3 outputs

print(torch.max(A, dim=0, keepdims=True)) # -> keep the dimensions
print(torch.max(A, dim=1, keepdims=True))

print(torch.argmax(A)) # index of max element
print(torch.argmax(A, dim=0)) # index of largest element in each column
print(torch.argmax(A, dim=1)) # index of largest element in each row
```

**Output:**
```
tensor([[-1.5949, -0.1680,  0.1829, -1.1441],
        [-1.0000, -0.8057,  1.1329,  0.5490],
        [-0.8281, -1.6320,  1.0594, -0.7965]])
tensor(1.1329)
torch.return_types.max(
values=tensor([-0.8281, -0.1680,  1.1329,  0.5490]),
indices=tensor([2, 0, 1, 1]))
torch.return_types.max(
values=tensor([0.1829, 1.1329, 1.0594]),
indices=tensor([2, 2, 2]))
```

## Sort, sum, mean, and standard deviation

### Sorting tensors

```python
a = torch.randn(6,1)
print(a)
a_sorted = torch.sort(a, dim=0) # ascending order
print(a_sorted)

print(a.sort(dim=0)) # alternative method for sorting
print(a.sort(dim=0, descending=True)) # descending order

print(torch.max(a))
print(a.max())

print(torch.abs(a))
print(a.abs())
```

### Statistical operations

```python
A = torch.randn(3,4)
print(A)

# Sum
print(torch.sum(A))
print(torch.sum(A, dim=1))
print(torch.sum(A, dim=1, keepdims=True))

# Mean
print(torch.mean(A))
print(torch.mean(A, dim=1))
print(torch.mean(A, dim=1, keepdim=True))

# Standard deviation
print(torch.std(A)) # Standard deviation

# Alternative method
print(A.sum(dim=1, keepdims=True))
print(A.mean(dim=1, keepdims=True))
print(A.std())
```

## Reshape operations

### Basic reshaping

```python
A = torch.randint(1, 5, size=(12,)) # From 1, less than 5, 12 integers (one dimension expression: (N,))
print(A)
print(A.shape)

B = A.reshape(2, 2, 3) # 2 dimensions, 2 rows, 3 columns
print(B)
print(B.ndim) # 3D array
```

**Output:**
```
tensor([1, 3, 3, 2, 2, 4, 4, 2, 4, 4, 1, 2])
torch.Size([12])
tensor([[[1, 3, 3],
         [2, 2, 4]],

        [[4, 2, 4],
         [4, 1, 2]]])
3
```

### Using -1 for automatic dimension inference

```python
A = torch.arange(20,)
print(A)
print(A.reshape(4, 5))
print(A.reshape(4, -1).shape) # Meaning of -1: Choose columns automatically
print(A.reshape(2, -1, 5).shape)
print(A.reshape(1, -1).shape) # 2D row vector
print(A.reshape(-1, 1).shape) # 2D column vector
```

**Output:**
```
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])
tensor([[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19]])
torch.Size([4, 5])
torch.Size([2, 2, 5])
torch.Size([1, 20])
torch.Size([20, 1])
```

## Transpose and permute

### Transpose operations

```python
a = torch.tensor([1,2,3])
b = torch.tensor([2,2,1])
print(torch.sum(a*b))

a = a.reshape(3,1)
b = b.reshape(3,1)
print(a.transpose(1,0)@b)
print(a.permute(1,0)@b)  # 0th dimension to 1st, 1st to 0th -> swap positions
print(a.T@b)
print(a.t()@b)

A = torch.randn(4,3,6)
print(A.permute(1,2,0).shape)
print(A.transpose(1,2).shape)
```

**Output:**
```
tensor(9)
tensor([[9]])
tensor([[9]])
torch.Size([3, 6, 4])
torch.Size([4, 6, 3])
```

### Ellipsis indexing

```python
x = torch.randn(2,3,4,5,6)
print(x.shape)
print(x[1,:,:,3,:].shape)
print(x[1,...,3,:].shape) # ... represents all dimensions in between
```

**Output:**
```
torch.Size([2, 3, 4, 5, 6])
torch.Size([3, 4, 6])
torch.Size([3, 4, 6])
```

## Stacking and concatenation

### Vertical and horizontal stacking

```python
A = torch.ones(3,4)
B = torch.zeros(3,4)

C = torch.vstack([A,B]) # vertical stack
D = torch.hstack([A,B]) # horizontal stack

E = torch.cat([A,B], dim=0) # Same as vstack
F = torch.cat([A,B], dim=1) # Same as hstack

print(C)
print(D)
print(E)
print(F)
```

**Output:**
```
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
tensor([[1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.]])
```

## Squeeze and unsqueeze

### Removing dimensions with squeeze

```python
A = torch.randn(1,1,1,3,1,1,1,4,1,1,1)
print(A.shape)
print(A.squeeze().shape) # remove dimension 1
print(A.squeeze(dim=(0,2,4,5)).shape) # remove dimensions with specific indices
```

**Output:**
```
torch.Size([1, 1, 1, 3, 1, 1, 1, 4, 1, 1, 1])
torch.Size([3, 4])
torch.Size([1, 3, 1, 4, 1, 1, 1])
```

### Adding dimensions with unsqueeze

```python
A = torch.randn(3,4)
print(A.unsqueeze(dim=0).shape)
print(A.unsqueeze(dim=1).shape)
print(A.unsqueeze(dim=2).shape)

print(A.reshape(1,3,4).shape)
print(A.reshape(3,1,4).shape)
print(A.reshape(3,4,1).shape)
```

**Output:**
```
torch.Size([1, 3, 4])
torch.Size([3, 1, 4])
torch.Size([3, 4, 1])
torch.Size([1, 3, 4])
torch.Size([3, 1, 4])
torch.Size([3, 4, 1])
```

### Using unsqueeze for concatenation

```python
A = torch.ones(3,4)
B = torch.zeros(3,4)
A = A.unsqueeze(dim=0)
B = B.unsqueeze(dim=0)
C = torch.cat([A,B], dim=0)
print(C)
print(C.shape)
```

**Output:**
```
tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
torch.Size([2, 3, 4])
```

### Stack operation

```python
A = torch.ones(3,4)
B = torch.zeros(3,4)
C = torch.stack([A,B])

print(C)
```

**Output:**
```
tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
```

## Clone operation

Cloning creates a new tensor that doesn't share memory with the original.

```python
A = torch.tensor([[1,2],[3,4]])
B = A.clone() # B=A will clone the address
B[0,0] = 100

print(B)
print(A)
```

**Output:**
```
tensor([[100,   2],
        [  3,   4]])
tensor([[1, 2],
        [3, 4]])
```

## Matrix multiplication for 3D arrays

```python
A = torch.randn(5,7)
B = torch.randn(7,10)
C = A@B
print(C.shape)

A = torch.randn(32,5,7)
B = torch.randn(32,7,10)
C = A@B
print(C.shape)
```

**Output:**
```
torch.Size([5, 10])
torch.Size([32, 5, 10])
```

## Converting between NumPy and PyTorch

```python
import numpy as np
import torch

a = np.array([1,2,3])
b = torch.tensor([1,2,3])

A = torch.tensor(a)
B = b.numpy()

print(type(A))
print(type(B))
```

**Output:**
```
<class 'torch.Tensor'>
<class 'numpy.ndarray'>
```

## Summary

This tutorial covered essential PyTorch operations:

- **Random generation**: `randn()` for normal distribution, `rand()` for uniform distribution
- **Mathematical functions**: `abs()`, `sqrt()`, `exp()`, `log()`, trigonometric functions
- **Aggregate functions**: `max()`, `min()`, `sum()`, `mean()`, `std()`
- **Reshaping**: `reshape()` with automatic dimension inference using -1
- **Transpose**: `transpose()`, `permute()`, `T`, `t()`
- **Stacking**: `vstack()`, `hstack()`, `cat()`, `stack()`
- **Dimension manipulation**: `squeeze()`, `unsqueeze()`
- **Copying**: `clone()` for creating independent copies
- **Conversions**: Moving between NumPy arrays and PyTorch tensors

These operations form the foundation for data preprocessing and manipulation in deep learning pipelines.
