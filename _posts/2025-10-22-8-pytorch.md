---
layout: single
title: "[PyTorch] 6. Binary Classification with PyTorch: Building a Multi-Layer Perceptron" 
date: 2025-10-22
categories: PyTorch
#header:
  #overlay_image: /assets/images/docker-container-tech.svg
  #overlay_filter: 0.5
  #teaser: /assets/images/docker-container-tech.svg
  #caption: "Create Docker Image"
Typora-root-url: ../
---
# Binary Classification with PyTorch: Building a Multi-Layer Perceptron

Binary classification is one of the most fundamental tasks in machine learning. This tutorial demonstrates how to build a Multi-Layer Perceptron (MLP) using PyTorch to classify data into two categories, visualize decision boundaries, and evaluate model performance.

## Step 1: Creating synthetic data

Let's generate synthetic data for a binary classification problem.

```python
import torch

torch.manual_seed(42)
# Generate two classes of data
N = 20
random0 = torch.randn(int(N/2), 1)   # Mean around 0
random5 = torch.randn(int(N/2), 1)+5 # Mean around 5

class1_data = torch.hstack([random0, random5]) # (10,2) data => blue points
class2_data = torch.hstack([random5, random0]) # (10,2) data => red points

class1_label = torch.ones(int(N/2), 1)
class2_label = torch.zeros(int(N/2), 1)

X = torch.vstack([class1_data, class2_data])
y = torch.vstack([class1_label, class2_label])
```

### Visualizing the data

```python
import matplotlib.pyplot as plt

plt.plot(class1_data[:, 0], class1_data[:, 1], 'o', label='Class 1')
plt.plot(class2_data[:, 0], class2_data[:, 1], 'ro', label='Class 2')
plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.grid()
```

The data consists of:
- **Class 1** (label=1): Blue points clustered around (0, 5)
- **Class 2** (label=0): Red points clustered around (5, 0)

## Step 2: Building the model

We'll create a Multi-Layer Perceptron with hidden layers and sigmoid activation functions.

```python
from torch import nn

class MLP(nn.Module):
    def __init__(self):
        super().__init__()

        # Multi-layer perceptron
        self.linear = nn.Sequential(nn.Linear(2, 100),
                                    nn.Sigmoid(),
                                    nn.Linear(100, 1),
                                    nn.Sigmoid())

    def forward(self, x):
        x = self.linear(x)
        return x
```

### Model architecture

```python
model = MLP()
print(model)
print(model(torch.randn(5, 2)).shape) # 5 data points, 2 features (x1, x2)
print(model(torch.randn(5, 2)))
```

**Output:**
```
MLP(
  (linear): Sequential(
    (0): Linear(in_features=2, out_features=100, bias=True)
    (1): Sigmoid()
    (2): Linear(in_features=100, out_features=1, bias=True)
    (3): Sigmoid()
  )
)
torch.Size([5, 1])
tensor([[0.4171],
        [0.4157],
        [0.4185],
        [0.4114],
        [0.4372]], grad_fn=<SigmoidBackward0>)
```

**Architecture breakdown:**
- **Input layer**: 2 features (x1, x2)
- **Hidden layer**: 100 neurons with sigmoid activation
- **Output layer**: 1 neuron with sigmoid activation (probability)

## Step 3: Training the model

### Training configuration

```python
from torch import optim

LR = 1e-1
EPOCH = 100

optimizer = optim.SGD(model.parameters(), lr=LR)
criterion = nn.BCELoss() # Binary Cross Entropy Loss

loss_history = []
grad_history = []
update_size_history = []
```

### Training loop

```python
model.train()
for ep in range(EPOCH):
    # Forward pass
    y_hat = model(X)
    loss = criterion(y_hat, y)

    # Track gradient information
    prev = model.linear[0].weight.detach().clone()

    # Backward pass
    optimizer.zero_grad() # gradient accumulation prevention
    loss.backward() # backpropagation
    optimizer.step() # weight update

    # Record metrics
    grad_history += [torch.sum(torch.abs(model.linear[0].weight.grad)).item()]
    update_size_history += [torch.sum(torch.abs(model.linear[0].weight.detach()-prev)).item()]

    loss_history += [loss.item()]
    print(f"Epoch: {ep+1}, train loss: {loss.item():.4f}")
    print("-"*20)
```

**Training output (abbreviated):**
```
Epoch: 1, train loss: 0.6443
--------------------
Epoch: 2, train loss: 0.5402
--------------------
...
Epoch: 99, train loss: 0.0210
--------------------
Epoch: 100, train loss: 0.0208
--------------------
```

### Visualizing training progress

```python
plt.plot(range(1, EPOCH+1), loss_history)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss over Epochs")
```

The loss decreases from ~0.64 to ~0.02, indicating successful learning.

## Step 4: Evaluating and visualizing results

### Creating a test grid

```python
x1_test = torch.linspace(-10, 10, 100) # 100 points from -10 to 10
x2_test = torch.linspace(-10, 10, 100)

X1_test, X2_test = torch.meshgrid(x1_test, x2_test, indexing='ij')
X_test = torch.cat([X1_test.unsqueeze(dim=2), X2_test.unsqueeze(dim=2)], dim=2)
```

### Making predictions

```python
# Set model to evaluation mode
model.eval()

with torch.no_grad(): # Disable gradient computation for efficiency
    y_hat = model(X_test)
    # Predictions on training data
    y_pred_class1 = model(class1_data)
    y_pred_class2 = model(class2_data)

Y_hat = y_hat.squeeze()
```

### 2D decision boundary visualization

```python
fig = plt.figure(figsize=(16, 14))

# 1. Contour plot with decision boundary
ax1 = plt.subplot(2, 2, 1)
contour = ax1.contourf(
    X1_test.numpy(), X2_test.numpy(), Y_hat.numpy(),
    levels=20, cmap='RdYlBu_r', alpha=0.8
)
ax1.contour(
    X1_test.numpy(), X2_test.numpy(), Y_hat.numpy(),
    levels=[0.5], colors='black', linewidths=2
)
ax1.scatter(class1_data[:, 0], class1_data[:, 1],
            c='blue', marker='o', s=100,
            edgecolors='black', label='Class 1 (label=1)', zorder=5)
ax1.scatter(class2_data[:, 0], class2_data[:, 1],
            c='red', marker='o', s=100,
            edgecolors='black', label='Class 2 (label=0)', zorder=5)
ax1.set_xlabel('x1', fontsize=12)
ax1.set_ylabel('x2', fontsize=12)
ax1.set_title('2D Decision Boundary (Contour)', fontsize=14, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3)
plt.colorbar(contour, ax=ax1, label='Prediction Probability')
```

### 3D surface visualization

```python
# 2. 3D surface plot
ax2 = plt.subplot(2, 2, 2, projection='3d')
ax2.view_init(elev=25, azim=140)
surf = ax2.plot_surface(
    X1_test.numpy(), X2_test.numpy(), Y_hat.numpy(),
    cmap='viridis', alpha=0.6, edgecolor='none'
)
ax2.scatter(class1_data[:, 0], class1_data[:, 1],
            y_pred_class1.squeeze(), c='blue', marker='o',
            s=100, edgecolors='black', label='Class 1')
ax2.scatter(class2_data[:, 0], class2_data[:, 1],
            y_pred_class2.squeeze(), c='red', marker='o',
            s=100, edgecolors='black', label='Class 2')
ax2.set_xlabel('x1', fontsize=10)
ax2.set_ylabel('x2', fontsize=10)
ax2.set_zlabel('Prediction', fontsize=10)
ax2.set_title('3D Decision Surface', fontsize=14, fontweight='bold')
ax2.legend(fontsize=9)
fig.colorbar(surf, ax=ax2, shrink=0.5, aspect=5)
```

### Confidence regions

```python
# 4. Confidence regions
ax4 = plt.subplot(2, 2, 4)
contour4 = ax4.contourf(
    X1_test.numpy(), X2_test.numpy(), Y_hat.numpy(),
    levels=[0, 0.3, 0.7, 1.0],
    colors=['#ff6b6b', '#ffffcc', '#6b9eff'], alpha=0.6
)
ax4.contour(
    X1_test.numpy(), X2_test.numpy(), Y_hat.numpy(),
    levels=[0.5], colors='black', linewidths=3, linestyles='--'
)
ax4.scatter(class1_data[:, 0], class1_data[:, 1],
            c='blue', marker='o', s=100,
            edgecolors='black', label='Class 1 (label=1)', zorder=5)
ax4.scatter(class2_data[:, 0], class2_data[:, 1],
            c='red', marker='o', s=100,
            edgecolors='black', label='Class 2 (label=0)', zorder=5)
ax4.set_xlabel('x1', fontsize=12)
ax4.set_ylabel('x2', fontsize=12)
ax4.set_title('Confidence Regions', fontsize=14, fontweight='bold')
ax4.legend(fontsize=10)
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Step 5: Evaluating model performance

```python
with torch.no_grad():
    train_pred = model(X)
    train_pred_class = (train_pred > 0.5).float()
    accuracy = (train_pred_class == y).float().mean()

    print(f"\nTraining data accuracy: {accuracy.item() * 100:.2f}%")
    print(f"Class 1 average prediction: {y_pred_class1.mean().item():.4f}")
    print(f"Class 2 average prediction: {y_pred_class2.mean().item():.4f}")
```

**Output:**
```
Training data accuracy: 100.00%
Class 1 average prediction: 0.9845
Class 2 average prediction: 0.0123
```

## Understanding the results

### Decision boundary

The black line at prediction=0.5 represents the decision boundary:
- **Above the line**: Model predicts Class 1 (blue)
- **Below the line**: Model predicts Class 2 (red)

### Prediction probabilities

- **Class 1 points**: Model outputs ~0.98 (high confidence)
- **Class 2 points**: Model outputs ~0.01 (high confidence for opposite class)

### Confidence regions

- **Red region**: Strong Class 2 prediction (p < 0.3)
- **Yellow region**: Uncertain (0.3 < p < 0.7)
- **Blue region**: Strong Class 1 prediction (p > 0.7)

## Key components explained

### Binary Cross Entropy Loss

```python
criterion = nn.BCELoss()
```

BCE Loss measures the difference between predicted probabilities and true labels:

$$\text{BCE} = -\frac{1}{N}\sum_{i=1}^{N}[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$$

- Penalizes confident wrong predictions heavily
- Works well with sigmoid output layer

### Sigmoid activation

The final sigmoid layer outputs probabilities between 0 and 1:

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

### Model evaluation modes

```python
model.train()  # Enable training mode (affects dropout, batch norm)
model.eval()   # Enable evaluation mode
```

## Best practices for binary classification

1. **Use sigmoid for output layer**: Ensures predictions are probabilities [0, 1]
2. **Use BCELoss**: Appropriate for binary classification
3. **Monitor training loss**: Ensure it decreases over epochs
4. **Visualize decision boundaries**: Helps understand model behavior
5. **Use `model.eval()` and `torch.no_grad()`**: For efficient inference
6. **Set random seed**: For reproducible results

## Improving model performance

### Techniques to try

1. **Increase hidden layer size**: More neurons for complex patterns
2. **Add more layers**: Deeper networks for more expressiveness
3. **Try different activation functions**: ReLU, LeakyReLU, Tanh
4. **Adjust learning rate**: Balance between speed and stability
5. **Add regularization**: Dropout, weight decay to prevent overfitting
6. **Normalize inputs**: Scale features to similar ranges

### Example: ReLU activation

```python
class ImprovedMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Sequential(
            nn.Linear(2, 100),
            nn.ReLU(),
            nn.Linear(100, 50),
            nn.ReLU(),
            nn.Linear(50, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.linear(x)
```

## Summary

This tutorial covered:

- **Data generation**: Creating synthetic binary classification data
- **Model architecture**: Building an MLP with `nn.Sequential`
- **Training loop**: Forward pass, loss computation, backpropagation
- **Visualization**: 2D contours, 3D surfaces, confidence regions
- **Evaluation**: Computing accuracy and prediction probabilities

### Complete workflow

1. **Prepare data**: Features (X) and labels (y)
2. **Define model**: MLP with sigmoid output
3. **Set up training**: Optimizer (SGD) and loss (BCELoss)
4. **Train model**: Iterate through epochs
5. **Evaluate**: Visualize decision boundaries and compute accuracy

This pattern extends to more complex classification tasks with multiple classes (using softmax and CrossEntropyLoss) and real-world datasets. Understanding binary classification is fundamental to mastering deep learning.