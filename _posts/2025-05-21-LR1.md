---
layout: post
title:  "Linear Regression 1 (Model, Cost Function, Gradient Descent)"
use_math: true
---

<script type="text/javascript"
  async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

# Notation
Here is a summary of some of the notation you will encounter.  

| General <img width=70/> <br /> Notation <img width=70/> | Description <img width=350/>                                                                            | Python (if applicable) |
| :------------------------------------------------------ | :------------------------------------------------------------------------------------------------------ | :--------------------- |
| $a$                                                     | Scalar, non-bold                                                                                        |                        |
| $\mathbf{a}$                                            | Vector, bold                                                                                            |                        |
| **Regression**                                          |                                                                                                         |                        |
| $\mathbf{x}$                                            | Training Example feature values                                                                         | `x_train`              |
| $\mathbf{y}$                                            | Training Example targets                                                                                | `y_train`              |
| $x^{(i)}$, $y^{(i)}$                                    | $i_{th}$ Training Example                                                                               | `x_i`, `y_i`           |
| $m$                                                     | Number of training examples                                                                             | `m`                    |
| $w$                                                     | Parameter: weight                                                                                       | `w`                    |
| $b$                                                     | Parameter: bias                                                                                         | `b`                    |
| $f_{w,b}(x^{(i)})$                                      | The result of the model evaluation at $x^{(i)}$ parameterized by $w,b$: $f_{w,b}(x^{(i)}) = wx^{(i)}+b$ | `f_wb`                 |

\n

# 1. What is Linear Regression Model?

Linear regression is one of the simplest yet most powerful algorithms in the field of data science. It predicts a continuous output based on one or more input features by fitting a straight line through the data. The model assumes a linear relationship between the independent variable(s) (features) and the dependent variable (target).

For example, if we want to predict housing prices based on house size, our dataset might look like this:

| Size (1000 sqft) | Price (1000s of dollars) |
| ---------------- | ------------------------ |
| 1.0              | 300                      |
| 2.0              | 500                      |

In this case, the house size is the independent variable ($x$), and the price is the dependent variable ($y$). Each training example can be denoted as $(x^{(i)}, y^{(i)})$, where the superscript $(i)$ refers to the $i^{th}$ example. For the dataset above:

- $(x^{(0)}, y^{(0)}) = (1.0, 300.0)$
- $(x^{(1)}, y^{(1)}) = (2.0, 500.0)$
    
Since there is only one independent variable, this is known as **univariate linear regression**.

# 2. Linear Regression Model Function

The linear regression model function is represented as
$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{1}$$
Where:

- $w$ is the **weight** (slope of the line).
- $b$ is the **bias** (intercept).
- $x^{(i)}$ represents the feature value.
    
Different values of $w$ and $b$ result in different lines on the plot. Using the housing price example:

- $f_{w,b}(x^{(0)}) = w \times 1.0 + b = w + b$
- $f_{w,b}(x^{(1)}) = w \times 2.0 + b = 2w + b$
    
To determine the optimal values of $w$ and $b$, we need to minimize the difference between the predicted values $f_{w,b}(x^{(i)})$ and the actual target values $y^{(i)}$. This is achieved using the **cost function**.

# 3. Cost Function 

The **cost function** quantifies how well our model predictions match the actual target values. For linear regression, the most commonly used cost function is the **Mean Squared Error (MSE)**:
$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \tag{2}$
Where:

- $f_{w,b}(x^{(i)}) = wx^{(i)} + b$ represents the predicted value for example $i$.
- $y^{(i)}$ is the actual target value.
- $m$ is the number of training examples.
- The factor $\frac{1}{2m}$ simplifies gradient calculations.

Our goal is to find the values of $w$ and $b$ that minimize the cost function:

$$
\underset{w,b}{\text{minimize}} J(w,b)
$$

The cost function creates a convex surface, where the lowest point represents the optimal values of $w$ and $b$.
![[Figure 2.png]]
# 4. Gradient Descent 

**Gradient descent** is an optimization algorithm used to minimize the cost function by iteratively updating the model parameters ($w$ and $b$). The process follows these steps:

1. **Initialize:** Start with arbitrary values of $w$ and $b$.
2. **Compute Gradient:** Calculate the partial derivatives of $J(w,b)$ with respect to $w$ and $b$.
3. **Update Parameters:** Adjust $w$ and $b$ in the direction of the negative gradient.
4. **Repeat:** Continue until the cost function converges to a minimum.
    
The parameter update rules are given by:

$$\begin{align*} \text{repeat}&\text{ until convergence:} \; \lbrace \newline
\;  w &= w -  \alpha \frac{\partial J(w,b)}{\partial w} \tag{3}  \; \newline 
 b &= b -  \alpha \frac{\partial J(w,b)}{\partial b}  \newline \rbrace
\end{align*}$$
Where:

- $\alpha$ is the **learning rate**, controlling the step size.
- $\frac{\partial J(w,b)}{\partial w}$ and $\frac{\partial J(w,b)}{\partial b}$ are the gradients of the cost function:

$$
\begin{align}
\frac{\partial J(w,b)}{\partial w}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \tag{4}\\
  \frac{\partial J(w,b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \tag{5}\\
\end{align}
$$
**Simultaneous Update:** It's important to update $w$ and $b$ simultaneously:
$$
\begin{aligned}
\textcolor{blue}{tmp\_w} &= w - \alpha \frac{\partial}{\partial w} J(w,b) \\
\textcolor{blue}{tmp\_b} &= b - \alpha \frac{\partial}{\partial b} J(w,b) \\
w &= tmp\_w \\
b &= tmp\_b
\end{aligned}
$$

Following wrong update logic can give totally different output for your model:
$$
\begin{aligned}
\textcolor{blue}{tmp\_w} &= w - \alpha \frac{\partial}{\partial w} J(w,b) \\
w &= tmp\_w \\
\textcolor{blue}{tmp\_b} &= b - \alpha \frac{\partial}{\partial b} J(w,b) \\

b &= tmp\_b
\end{aligned}
$$

Here are some simple visual explanations how gradient descent algorithm works:

![[Draft Note.jpeg]]

where
$$
w = w-\alpha\frac{\partial J(w)}{\partial w} \tag{6}\\
$$Here, we set bias ($b$) as 0 and $\alpha$ is learning rate.

If $\alpha$ is too small, gradient descent would be too slow:

![[Draft Note 2.jpeg]]

On the other hand, if it is too big, it will overshoot, never reach maximum or fail to converge and diverge:

![[스크린샷, 2025-02-18 오후 6.11.02.jpeg]]

In summary:

**Learning Rate ($\alpha$)**
The learning rate determines the step size of each update:

- **Too small:** Slow convergence, taking many iterations.
- **Too large:** Overshooting the minimum, potentially diverging.
    
Therefore, selecting an appropriate learning rate is crucial for efficient training.

---
# 5. Conclusion

Linear regression provides a simple yet powerful approach to modeling relationships between variables. By understanding the model, cost function, and gradient descent optimization, you can effectively train linear regression models for various tasks.

Key takeaways:

1. **Model:** Linear equation $f_{w,b}(x) = wx + b$.
2. **Cost Function:** Mean squared error $J(w,b)$.
3. **Optimization:** Gradient descent to minimize $J(w,b)$.
    
Linear regression serves as a foundation for more complex algorithms, making it an essential tool for any data scientist.

---
# 6. Reference

This material is based on DeepLearning.AI's _Supervised Machine Learning: Regression and Classification_ course, week1. (https://www.coursera.org/learn/machine-learning/home/week/1)
























