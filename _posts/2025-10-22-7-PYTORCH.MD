---
layout: single
title: "[PyTorch] 5. Building Neural Networks with PyTorch nn.Module" 
date: 2025-10-22
categories: PyTorch
#header:
  #overlay_image: /assets/images/docker-container-tech.svg
  #overlay_filter: 0.5
  #teaser: /assets/images/docker-container-tech.svg
  #caption: "Create Docker Image"
Typora-root-url: ../
---

# Building Neural Networks with PyTorch nn.Module

PyTorch's `nn.Module` is the foundation for building neural networks. This tutorial will teach you how to create both simple and complex neural network architectures, manage parameters, and understand the structure of deep learning models.

## Understanding nn.Linear

The `nn.Linear` layer implements a fully connected (dense) layer with learnable weights and bias.

```python
import torch
from torch import nn
from torchviz import make_dot

x = torch.tensor([1.])
model = nn.Linear(1, 1) # 1 input node and 1 output node
print(model)

print(model.weight)
print(model.bias)

y = model(x)
print(y)

y2 = x @ model.weight + model.bias
print(y2)

make_dot(y)
```

**Output:**
```
Linear(in_features=1, out_features=1, bias=True)
Parameter containing:
tensor([[-0.3042]], requires_grad=True)
Parameter containing:
tensor([-0.6220], requires_grad=True)
tensor([-0.9263], grad_fn=<ViewBackward0>)
tensor([-0.9263], grad_fn=<AddBackward0>)
```

**Key points:**
- `nn.Linear(in_features, out_features)` creates a linear transformation: $y = xW^T + b$
- Weights and biases are automatically initialized and require gradients
- The computation is equivalent to matrix multiplication plus bias

## Multi-layer networks

You can stack multiple linear layers to create deeper networks.

```python
fc1 = nn.Linear(1, 3) # 3 weights, 3 bias
fc2 = nn.Linear(3, 1) # 3 weights, 1 bias

print(fc1.weight)
print(fc1.bias)

print(fc2.weight)
print(fc2.bias)

x = torch.tensor([1.])
x = fc1(x)
print(x)

y = fc2(x)
print(y)

x = torch.tensor([1.])
y2 = (x @ fc1.weight.T + fc1.bias) @ fc2.weight.T + fc2.bias
print(y2)

make_dot(y)
```

**Output:**
```
Parameter containing:
tensor([[-0.1574],
        [ 0.1835],
        [ 0.5685]], requires_grad=True)
Parameter containing:
tensor([-0.5569,  0.3481,  0.2973], requires_grad=True)
Parameter containing:
tensor([[ 0.2219, -0.2158, -0.1962]], requires_grad=True)
Parameter containing:
tensor([0.5302], requires_grad=True)
tensor([-0.7143,  0.5315,  0.8659], grad_fn=<ViewBackward0>)
tensor([0.0872], grad_fn=<ViewBackward0>)
```

## Working with batches

Neural networks typically process multiple samples at once (batches).

### Single sample

```python
model = nn.Linear(2, 3)
x = torch.randn(2) # input should be 1D data
print(x)
print(model(x))
make_dot(model(x))
```

**Output:**
```
tensor([-0.9514,  0.2281])
tensor([0.4959, 0.6121, 0.6999], grad_fn=<ViewBackward0>)
```

### Batch processing

```python
model = nn.Linear(2, 3)
# Multiple data samples in (batch_size, features) format
x = torch.randn(5, 2) # 5 samples with 2 features each (e.g., height, weight)
print(x)
print(model(x))

x = torch.randn(4, 5, 2) # 4 groups of 5 samples with 2 features
print(model(x).shape)  # output => [4, 5, 3]

make_dot(model(x))
```

**Output:**
```
tensor([[-0.1509,  0.3751],
        [ 0.8407, -2.9230],
        [-0.0908, -0.8833],
        [ 1.7018, -0.3735],
        [-1.9652,  0.7945]])
tensor([[ 0.1995,  0.0417, -0.3001],
        [-1.0239, -0.7678,  2.5196],
        [-0.1963, -0.4250,  0.6189],
        [-0.4415,  0.6650,  1.1420],
        [ 0.7318, -0.6925, -1.4907]], grad_fn=<AddmmBackward0>)
torch.Size([4, 5, 3])
```

**Key insight:** Linear layers operate on the last dimension, preserving all other dimensions.

## Building models with nn.Sequential

`nn.Sequential` provides a simple way to stack layers in sequence.

```python
model = nn.Sequential(nn.Linear(2, 5),
                      nn.Linear(5, 10),
                      nn.Linear(10, 3))

x = torch.randn(5, 2)

print(x)
print(model(x))
```

**Output:**
```
tensor([[-0.7277, -1.8625],
        [-0.7128, -0.6964],
        [ 0.2854, -0.2188],
        [ 1.5054, -0.2051],
        [-1.5670,  1.2422]])
tensor([[-0.5339, -0.0179,  0.3858],
        [-0.4938, -0.0113,  0.3131],
        [-0.2718, -0.0664,  0.2906],
        [-0.0187, -0.1374,  0.2987],
        [-0.6091,  0.0509,  0.1856]], grad_fn=<AddmmBackward0>)
```

## Creating custom models with nn.Module

For more complex architectures, create custom classes inheriting from `nn.Module`.

### Method 1: Explicit layer definition

```python
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()

        self.fc1 = nn.Linear(2, 5)
        self.fc2 = nn.Linear(5, 10)
        self.fc3 = nn.Linear(10, 3)
        self.act = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        x = self.act(x)
        x = self.fc3(x)
        x = self.act(x)
        return x

model = MyModel()
x = torch.randn(5, 2)
y = model(x)
print(y)

make_dot(y)
```

**Output:**
```
tensor([[0.4122, 0.4939, 0.4004],
        [0.4014, 0.4797, 0.4078],
        [0.4074, 0.4871, 0.4029],
        [0.4032, 0.4822, 0.4064],
        [0.4060, 0.4851, 0.4035]], grad_fn=<SigmoidBackward0>)
```

### Accessing model parameters

```python
print(model)
print(model.fc1.weight)
print(model.fc2.bias)
```

**Output:**
```
MyModel(
  (fc1): Linear(in_features=2, out_features=5, bias=True)
  (fc2): Linear(in_features=5, out_features=10, bias=True)
  (fc3): Linear(in_features=10, out_features=3, bias=True)
  (act): Sigmoid()
)
Parameter containing:
tensor([[-0.3758, -0.6208],
        [ 0.5078,  0.3828],
        [ 0.6228, -0.3732],
        [-0.2359,  0.0621],
        [ 0.4261, -0.0204]], requires_grad=True)
Parameter containing:
tensor([ 0.3440, -0.4266, -0.2649,  0.0461, -0.0780, -0.1775, -0.1342, -0.1727,
         0.1028, -0.0128], requires_grad=True)
```

### Method 2: Using nn.Sequential inside nn.Module

```python
class MyModel2(nn.Module):
    def __init__(self):
        super().__init__()

        self.linear = nn.Sequential(nn.Linear(2, 5),
                                    nn.Sigmoid(),
                                    nn.Linear(5, 10),
                                    nn.Sigmoid(),
                                    nn.Linear(10, 3),
                                    nn.Sigmoid())

    def forward(self, x):
        x = self.linear(x)
        return x

model2 = MyModel2()
x = torch.randn(5, 2)
y = model2(x)
print(y)

make_dot(y)
```

**Output:**
```
tensor([[0.3302, 0.3574, 0.4487],
        [0.3287, 0.3588, 0.4486],
        [0.3319, 0.3549, 0.4508],
        [0.3290, 0.3594, 0.4471],
        [0.3324, 0.3551, 0.4492]], grad_fn=<SigmoidBackward0>)
```

### Accessing Sequential layers

```python
print(model2)
print(model2.linear[0].weight)
print(model2.linear[-2].bias) # Second to last layer
```

**Output:**
```
MyModel2(
  (linear): Sequential(
    (0): Linear(in_features=2, out_features=5, bias=True)
    (1): Sigmoid()
    (2): Linear(in_features=5, out_features=10, bias=True)
    (3): Sigmoid()
    (4): Linear(in_features=10, out_features=3, bias=True)
    (5): Sigmoid()
  )
)
Parameter containing:
tensor([[ 0.2846,  0.6374],
        [-0.5946,  0.5892],
        [-0.0164,  0.0541],
        [-0.2059, -0.0270],
        [-0.5614, -0.5610]], requires_grad=True)
Parameter containing:
tensor([-0.0908, -0.0813, -0.2849], requires_grad=True)
```

## Managing model parameters

### Listing all parameters

```python
list(model.parameters())
```

**Output:**
```
[Parameter containing:
 tensor([[-0.3758, -0.6208],
         [ 0.5078,  0.3828],
         [ 0.6228, -0.3732],
         [-0.2359,  0.0621],
         [ 0.4261, -0.0204]], requires_grad=True),
 Parameter containing:
 tensor([ 0.1921, -0.3896,  0.4714,  0.1387, -0.6752], requires_grad=True),
 ...]
```

### Counting total parameters

```python
# total parameter count
num = sum([p.numel() for p in model.parameters() if p.requires_grad])
print(num)
```

**Output:**
```
108
```

## Key architecture patterns

### When to use nn.Sequential

Use `nn.Sequential` when:
- Layers flow in a simple linear sequence
- No branching or skip connections
- No complex logic in forward pass

```python
simple_model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 10),
    nn.ReLU(),
    nn.Linear(10, 1)
)
```

### When to use custom nn.Module

Use custom `nn.Module` when:
- Complex forward pass logic
- Skip connections (ResNet-style)
- Multiple inputs/outputs
- Conditional computation

```python
class ComplexModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.branch1 = nn.Linear(10, 5)
        self.branch2 = nn.Linear(10, 5)
        self.combine = nn.Linear(10, 1)

    def forward(self, x):
        b1 = self.branch1(x)
        b2 = self.branch2(x)
        combined = torch.cat([b1, b2], dim=1)
        return self.combine(combined)
```

## Common activation functions

```python
# Common activation functions
nn.ReLU()      # Rectified Linear Unit: max(0, x)
nn.Sigmoid()   # Sigmoid: 1 / (1 + e^(-x))
nn.Tanh()      # Hyperbolic tangent
nn.LeakyReLU() # Leaky ReLU: max(0.01*x, x)
nn.Softmax()   # Softmax for multi-class classification
```

## Summary

This tutorial covered:

- **nn.Linear**: Fully connected layers with learnable weights and biases
- **Batch processing**: Processing multiple samples simultaneously
- **nn.Sequential**: Simple way to stack layers linearly
- **Custom nn.Module**: Creating complex architectures with custom logic
- **Parameter management**: Accessing and counting model parameters

### Key takeaways

1. **Always call `super().__init__()`** in custom `nn.Module` classes
2. **Define layers in `__init__()`** and use them in `forward()`
3. **Use nn.Sequential** for simple linear architectures
4. **Use custom nn.Module** for complex architectures
5. **Access parameters** with `.parameters()` for optimizers
6. **Layers operate on the last dimension**, preserving batch dimensions

Understanding `nn.Module` is essential for building any PyTorch model, from simple linear regression to complex transformers. This modular design makes PyTorch flexible and intuitive for both research and production.
