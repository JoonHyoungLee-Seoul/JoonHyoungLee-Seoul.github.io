---
layout: single
title: "[PyTorch] 4. Linear Regression with PyTorch: A Complete Training Example" 
date: 2025-10-22
categories: PyTorch
#header:
  #overlay_image: /assets/images/docker-container-tech.svg
  #overlay_filter: 0.5
  #teaser: /assets/images/docker-container-tech.svg
  #caption: "Create Docker Image"
Typora-root-url: ../
---

# Linear Regression with PyTorch: A Complete Training Example

Linear regression is the foundation of machine learning and a perfect starting point for understanding PyTorch's training workflow. This tutorial demonstrates how to build, train, and visualize a simple linear regression model.

## Setting up the problem

We'll create a simple dataset relating height to weight and train a model to learn this relationship.

```python
import torch
import matplotlib.pyplot as plt

x = torch.tensor([150, 160, 170, 175, 185.]) # height
y = torch.tensor([55, 70, 64, 80, 75.]) # weight
N = len(x)
plt.plot(x, y, 'o')
```

## Building and training the model

### Step 1: Data preparation

```python
from torch import nn, optim

x = torch.tensor([150, 160, 170, 175, 185.]) # height
y = torch.tensor([55, 70, 64, 80, 75.]) # weight
x = x.reshape(-1, 1) # rows: auto, columns: 1
y = y.reshape(-1, 1)
```

### Step 2: Model initialization

```python
model = nn.Linear(1, 1) # 1 input node, 1 output node
model.weight.data = torch.tensor([[0.45]])
model.bias.data = torch.tensor([-35.])
```

### Step 3: Training configuration

```python
LR = 3e-6
EPOCH = 20
optimizer = optim.SGD(model.parameters(), lr=LR)
criterion = nn.MSELoss()

loss_history = []
```

### Step 4: Training loop

```python
for ep in range(EPOCH):
    # Inference
    y_hat = model(x)

    # Compute loss
    loss = criterion(y_hat, y)

    # Update weights
    optimizer.zero_grad() # prevent gradient accumulation
    loss.backward() # backpropagation
    optimizer.step() # weight update

    # Record loss
    loss_history += [loss.item()]
    print(f"Epoch: {ep+1}, train loss: {loss.item():.4f}")

    # Print weight and bias
    print(f"Weight: {model.weight.data.item():.4f}, Bias: {model.bias.data.item():.4f}")

    # Visualize progress
    x_plot = torch.linspace(145, 190, 100)
    y_plot = model.weight.squeeze().detach().item() * x_plot + model.bias.detach().item()
    plt.figure()
    plt.plot(x, y, 'o')
    plt.plot(x_plot, y_plot, 'r')
    plt.title(f"Epoch {ep+1}")
    plt.show()

    print("-"*20)
```

## Understanding the training loop

Let's break down each component of the training loop:

### Forward pass (Inference)

```python
y_hat = model(x)
```

The model makes predictions based on the current weights and bias:
$$\hat{y} = wx + b$$

### Loss computation

```python
loss = criterion(y_hat, y)
```

Mean Squared Error (MSE) measures how far predictions are from actual values:
$$\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

### Gradient computation

```python
optimizer.zero_grad() # Clear previous gradients
loss.backward() # Compute gradients via backpropagation
```

**Important:** Always call `zero_grad()` before `backward()` to prevent gradient accumulation from previous iterations.

### Parameter update

```python
optimizer.step() # Update weights using computed gradients
```

The optimizer updates parameters using Stochastic Gradient Descent (SGD):
$$w_{new} = w_{old} - \text{lr} \times \frac{\partial \text{loss}}{\partial w}$$
$$b_{new} = b_{old} - \text{lr} \times \frac{\partial \text{loss}}{\partial b}$$

## Visualizing training progress

During training, you can visualize how the model's predictions improve:

```python
x_plot = torch.linspace(145, 190, 100)
y_plot = model.weight.squeeze().detach().item() * x_plot + model.bias.detach().item()

plt.figure()
plt.plot(x, y, 'o', label='Data points')
plt.plot(x_plot, y_plot, 'r', label='Model prediction')
plt.xlabel('Height')
plt.ylabel('Weight')
plt.legend()
plt.title(f"Epoch {ep+1}")
plt.show()
```

## Key components explained

### Optimizer

```python
optimizer = optim.SGD(model.parameters(), lr=LR)
```

The optimizer updates model parameters based on computed gradients. Common optimizers include:
- **SGD**: Stochastic Gradient Descent
- **Adam**: Adaptive Moment Estimation
- **RMSprop**: Root Mean Square Propagation

### Loss function

```python
criterion = nn.MSELoss()
```

The loss function measures how well the model performs. Common loss functions:
- **MSELoss**: Mean Squared Error (for regression)
- **CrossEntropyLoss**: For classification
- **BCELoss**: Binary Cross Entropy (for binary classification)

### Learning rate

```python
LR = 3e-6 # 0.000003
```

The learning rate controls the step size during optimization:
- **Too large**: Training may diverge
- **Too small**: Training will be very slow
- **Just right**: Smooth, efficient convergence

## Complete training workflow

1. **Initialize model**: Set up neural network architecture
2. **Define optimizer**: Choose optimization algorithm and learning rate
3. **Define loss function**: Select appropriate metric for your task
4. **Training loop**:
   - Forward pass: Make predictions
   - Compute loss: Measure prediction error
   - Backward pass: Compute gradients
   - Update parameters: Adjust weights to reduce loss
5. **Monitor progress**: Track loss and visualize results

## Best practices

1. **Always call `zero_grad()`**: Prevents gradient accumulation
2. **Monitor loss**: Track training progress over epochs
3. **Use appropriate learning rate**: Experiment with different values
4. **Visualize results**: Plot predictions to verify model behavior
5. **Reshape data properly**: Ensure inputs have correct dimensions

## Example output format

```
Epoch: 1, train loss: 125.4567
Weight: 0.4523, Bias: -34.8901
--------------------
Epoch: 2, train loss: 98.3421
Weight: 0.4789, Bias: -33.2156
--------------------
...
```

## Summary

This tutorial covered:
- **Data preparation**: Reshaping tensors for model input
- **Model initialization**: Setting up `nn.Linear`
- **Training configuration**: Optimizer, loss function, learning rate
- **Training loop**: Forward pass, loss computation, backpropagation, parameter update
- **Visualization**: Plotting training progress

Linear regression in PyTorch demonstrates the fundamental workflow you'll use for all deep learning models:
1. Define model architecture
2. Set up optimizer and loss function
3. Iterate through training loop
4. Monitor and visualize results

This same pattern applies to more complex models like CNNs, RNNs, and Transformers. Master this foundation, and you'll be well-prepared for advanced deep learning.
