---
layout: single
title: "[PyTorch] 1. PyTorch Basics: Tensors and Indexing" 
date: 2025-10-22
categories: PyTorch
#header:
  #overlay_image: /assets/images/docker-container-tech.svg
  #overlay_filter: 0.5
  #teaser: /assets/images/docker-container-tech.svg
  #caption: "Create Docker Image"
Typora-root-url: ../
---

# PyTorch Basics: Tensors and Indexing

PyTorch is a powerful deep learning framework that uses tensors as its fundamental data structure. This tutorial will walk you through creating tensors, performing basic computations, and mastering various indexing techniques.

## Creating tensors and basic computation

Tensors are the fundamental building blocks in PyTorch, similar to NumPy arrays but with GPU acceleration capabilities.

### Creating basic tensors

```python
import torch

a = torch.tensor([1,2,3,4]) # Function that uses list as input
print(a)
print(type(a))
print(a.dtype) # data type
print(a.shape)
```

**Output:**
```
tensor([1, 2, 3, 4])
<class 'torch.Tensor'>
torch.int64
torch.Size([4])
```

### Data type inference

```python
b = torch.tensor([1,2,3.1,4])
print(b.dtype) # if one is float, whole type is float
print(b)
```

**Output:**
```
torch.float32
tensor([1.0000, 2.0000, 3.1000, 4.0000])
```

### Multi-dimensional tensors

```python
A = torch.tensor([[1,2,3],[3,4,5]])
print(A)
print(A.shape)
print(A.ndim)
print(A.numel())
```

**Output:**
```
tensor([[1, 2, 3],
        [3, 4, 5]])
torch.Size([2, 3])
2
6
```

### Creating special tensors

```python
print(torch.zeros(5))
print(torch.zeros_like(A))
print(torch.ones(5))
print(torch.zeros(3,3))
print(torch.arange(3,10,2))   # Same as range, but in tensor
print(torch.arange(0,1,0.1))  # decimal OK
print(torch.linspace(0,1,10)) # From 0 to 1, into 10 pieces
```

**Output:**
```
tensor([0., 0., 0., 0., 0.])
tensor([[0, 0, 0],
        [0, 0, 0]])
tensor([1., 1., 1., 1., 1.])
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
tensor([3, 5, 7, 9])
tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000, 0.9000])
tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889, 1.0000])
```

### Basic tensor operations

```python
a = torch.tensor([1,2,3])
b = torch.tensor([4,5,6])

c = a + b

print(c)
```

**Output:**
```
tensor([5, 7, 9])
```

### Element-wise operations

```python
A = torch.tensor([[1,2,3],[1,2,3]])
B = torch.tensor([[4,5,6],[1,1,1]])
C = A+B
D = A-B

print(C)
print(D)

print()
print(A*B) # Hadamard Product
print(A/B)
print(B**2)
```

**Output:**
```
tensor([[5, 7, 9],
        [2, 3, 4]])
tensor([[-3, -3, -3],
        [ 0,  1,  2]])

tensor([[ 4, 10, 18],
        [ 1,  2,  3]])
tensor([[0.2500, 0.4000, 0.5000],
        [1.0000, 2.0000, 3.0000]])
tensor([[16, 25, 36],
        [ 1,  1,  1]])
```

### Matrix multiplication

```python
A = torch.tensor([[1,2],[3,4]])
B = torch.tensor([[1,2],[3,4]])

print(A*B) # Hadamard Product
print(A@B) # Matrix Product
```

**Output:**
```
tensor([[ 1,  4],
        [ 9, 16]])
tensor([[ 7, 10],
        [15, 22]])
```

## Basic indexing and slicing

PyTorch tensors support NumPy-style indexing and slicing.

### 1D tensor indexing

```python
a = torch.tensor([1,2,3,4,5,6,7,8,9])

# indexing and slicing
print(a[0]) # 1
print(a[1]) # 2
print(a[-1]) # 9
print(a[1:4]) # 2,3,4
print(a[7:]) # 8,9
print(a[:7]) # 1,2,3,4,5,6
print(a[:]) # 1,2,3,4,5,6,7,8,9
```

**Output:**
```
tensor(1)
tensor(2)
tensor(9)
tensor([2, 3, 4])
tensor([8, 9])
tensor([1, 2, 3, 4, 5, 6, 7])
tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
```

### 2D tensor indexing

```python
# 2 dimension series indexing

A = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])
print(A[0]) #[1,2,3]
print(A[-1]) # [7,8,9]
print(A[1:]) # [4,5,6]
print(A[:]) #[1,2,3],[4,5,6],[7,8,9]
print(A[0][2]) # 3
print(A[0,2]) # 3

B = [[1,2,3,4], [5,6,7,8]]
print(B[0][2]) # 3

print(A[1,:]) # [4,5,6]
print(A[1,0:3:2]) # 0:3, range:2 -> [4,6]
print(A[:,2]) # [3,6,9]
print(A[:][2]) # A[:]: A itself -> [7,8,9]
```

**Output:**
```
tensor([1, 2, 3])
tensor([7, 8, 9])
tensor([[4, 5, 6],
        [7, 8, 9]])
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
tensor(3)
tensor(3)
3
tensor([4, 5, 6])
tensor([4, 6])
tensor([3, 6, 9])
tensor([7, 8, 9])
```

### 3D tensor indexing

```python
# 3 dimension series indexing
A = torch.tensor([ [[0,1,2,3], [4,5,6,7], [8,9,10,11]],
                  [[12,13,14,15], [16,17,18,19],[20,21,22,23]]])

print(A)
print(A.shape) # order: [dimension, row, column]
print(A[0,1,2])

a = torch.tensor([1,2,3,4])
print(a.shape)
```

**Output:**
```
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
torch.Size([2, 3, 4])
tensor(6)
torch.Size([4])
```

## Advanced indexing techniques

### List-based indexing

```python
# indexing using list

A = torch.tensor([ [[0,1,2,3], [4,5,6,7], [8,9,10,11]],
                  [[12,13,14,15], [16,17,18,19],[20,21,22,23]]])

print(A)

# How to read -> [0,0,3], [1,1,3], ... first elements of the lists to last elements.
A[[0,1,1,0], [0,1,2,1], [3,3,2,1]]
```

**Output:**
```
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])

tensor([ 3, 19, 22,  5])
```

### Boolean indexing

```python
# indexing using boolean

a = [1,2,3,4,5,3,3]
print(a==3) # False
A = torch.tensor([[1,2,3,4],[5,3,7,3]])
print(A>3) # return boolean output for each element.
print(A[A>3]) # Indexing with true & false list.

A[A>3] = 100 # If number is greater than 3, change into 100.
print(A)

A = torch.tensor([[1,2],[3,4],[5,6],[7,8]])
B = torch.tensor([True, False, False, True])
print(A[B,:]) # output: [[1,2],[7,8]]

b = torch.tensor([1,2,3,4])
print(b[[True,True,False,False]])
```

**Output:**
```
False
tensor([[False, False, False,  True],
        [ True, False,  True, False]])
tensor([4, 5, 7])
tensor([[  1,   2,   3, 100],
        [100,   3, 100,   3]])
tensor([[1, 2],
        [7, 8]])
tensor([1, 2])
```

### Tensor-based indexing

```python
# Indexing with tensor

a = torch.tensor([1,2,3,4,5])
A = a[2]
print(A)

A = a[torch.tensor(2)] # torch.tensor inside tensor
print(A)

A = a[torch.tensor([2,3,4])]
print(A)

a = torch.tensor([1,2,3])
print(a[torch.tensor([[1,1,1], [2,2,2]])])

a = torch.tensor([[1,2,3],[4,5,6]])
print(a[torch.tensor(0)])

# [[[1,2,3],[4,5,6]],[[4,5,6],[4,5,6]]] -> 2 dimension, 2 rows, 3 columns (3 dimension)
A = a[torch.tensor([[0,1],[1,1]])]
print(A.shape)
print(A)
```

**Output:**
```
tensor(3)
tensor(3)
tensor([3, 4, 5])
tensor([[2, 2, 2],
        [3, 3, 3]])
tensor([1, 2, 3])
torch.Size([2, 2, 3])
tensor([[[1, 2, 3],
         [4, 5, 6]],

        [[4, 5, 6],
         [4, 5, 6]]])
```

### Practical example: Image segmentation

Tensor indexing is commonly used for tasks like image segmentation where you need to map indices to RGB values.

```python
# Usually used for image segmentation (single index to RGB)
b = torch.tensor([[255,255,0], [0,255,0], [0,0,255], [255,0,255], [70,80,75],[0,0,4],[60,100,255]])
import matplotlib.pyplot as plt
plt.imshow(b[torch.tensor([[0,1],[2,3]])])
```

## Challenge exercise

Try to replicate this tensor indexing result using list-based indexing:

```python
a = torch.tensor([[1,2,3], [4,5,6]])

A = a[torch.tensor([[0,1], [1,1]])]
print(A)

# Get output of a using list indexing:
B = a[[[[0,0,0], [1,1,1]], [[1,1,1], [1,1,1]]], [[[0,1,2], [0,1,2]], [[0,1,2], [0,1,2]]]]
print(B)
```

**Output:**
```
tensor([[[1, 2, 3],
         [4, 5, 6]],

        [[4, 5, 6],
         [4, 5, 6]]])
tensor([[[1, 2, 3],
         [4, 5, 6]],

        [[4, 5, 6],
         [4, 5, 6]]])
```

## Summary of indexing methods

```python
A = torch.tensor([[1,2,6],[3,4,7],[5,6,2],[7,8,9]])
print(A)
print(A.shape)

# 1. A[row index, column index]
print(A[0,1])

# 2-1. A[[row indices], [column indices]]
print(A[[0,2,3,1,2], [1,1,0,0,0]])

# 2-2. A[[[row indices], [row indices]], [[column indices], [column indices]]]
# Result will be in matrix form
print(A[[[0,2],[3,1]], [[0,2],[1,0]]])

# 3. A[tensor(bool)] => Boolean mask with same shape as A
print(A[torch.tensor([[False,True,True], [False,False,False], [False,False,True], [False,True,False]])])
print(A[A==2])

# 4. A[row boolean mask, column boolean mask]
print(A[[True, False, False, False], [False,True,True]]) # row 0, columns 1,2

# 5. A[tensor] => Stack elements based on indices
print(A[torch.tensor([1,1,2,2,2])])
```

**Output:**
```
tensor([[1, 2, 6],
        [3, 4, 7],
        [5, 6, 2],
        [7, 8, 9]])
torch.Size([4, 3])
tensor(2)
tensor([2, 6, 7, 3, 5])
tensor([[1, 2],
        [8, 3]])
tensor([2, 6, 2, 8])
tensor([2, 2])
tensor([2, 6])
tensor([[3, 4, 7],
        [3, 4, 7],
        [5, 6, 2],
        [5, 6, 2],
        [5, 6, 2]])
```

## Key takeaways

- **Tensors** are the fundamental data structure in PyTorch, similar to NumPy arrays
- **Basic indexing** uses square brackets with integers and slices
- **Advanced indexing** includes list-based, boolean, and tensor-based indexing
- **Element-wise operations** use standard operators (+, -, *, /, **)
- **Matrix multiplication** uses the @ operator
- Understanding indexing is crucial for data manipulation in deep learning tasks

This foundation will be essential as you progress to more advanced PyTorch operations and neural network implementations.
